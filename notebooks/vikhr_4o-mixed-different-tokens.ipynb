{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a2a2bf1-52f6-4f7c-8af4-367d98e456cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4477f9f-0bb3-4f85-b531-99123c2a7e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import wandb\n",
    "\n",
    "from tqdm import tqdm\n",
    "from multiprocess import set_start_method\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset, ConcatDataset\n",
    "from datasets import load_dataset, load_from_disk, Audio, concatenate_datasets\n",
    "from datasets import Value, DatasetDict\n",
    "\n",
    "import transformers\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForCausalLM,  \n",
    "    TrainingArguments, \n",
    "    default_data_collator, \n",
    "    get_scheduler\n",
    ")\n",
    "from accelerate import Accelerator\n",
    "\n",
    "from speechtokenizer import SpeechTokenizer\n",
    "from audiotools import AudioSignal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "048ca833-64c8-4c59-a932-64ff25498d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "save_dir = \"./results_asr_mixed_different_tokens\"\n",
    "\n",
    "data = \"librispeech\" # [\"parler-tts\", \"tedlium\", \"librispeech\"]\n",
    "\n",
    "start_audio_token = \"<soa>\"\n",
    "end_audio_token = \"<eoa>\"\n",
    "end_sequence_token = \"<eos>\"\n",
    "n_special_tokens = 3\n",
    "\n",
    "n_codebooks = 3\n",
    "max_seq_length = 2048\n",
    "\n",
    "device = 0\n",
    "load_processed = False\n",
    "path_to_processed = \"./data/processed/\"\n",
    "path_to_cache = \"./data/cache/\"\n",
    "quantize_before_training = False\n",
    "\n",
    "\n",
    "torch.cuda.set_device(f\"cuda:{device}\")\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f9bd0e8-1eea-4f1b-9697-9eb62b13f405",
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze_entire_model(model):\n",
    "    for n, p in model.named_parameters():\n",
    "        p.requires_grad = False\n",
    "    return model\n",
    "\n",
    "\n",
    "def freeze(\n",
    "    model,\n",
    "    freeze_emb=False,\n",
    "    freeze_ln=False,\n",
    "    freeze_attn=False,\n",
    "    freeze_ff=True,\n",
    "    freeze_ff_layers=[5,6,7,8,9,12,23,14,18,19,20,0,25],  # None means all or no layers, depending on freeze_ff\n",
    "    freeze_other=False,\n",
    "):\n",
    "    if freeze_ff_layers is not None and not isinstance(freeze_ff_layers, (list, set)):\n",
    "        raise ValueError(\"freeze_ff_layers must be a list or set of layer indices\")\n",
    "\n",
    "    for name, p in model.named_parameters():\n",
    "        name = name.lower()\n",
    "        layer_index = None\n",
    "        if 'mlp' in name:\n",
    "            # Parse the layer index from the parameter name if possible\n",
    "            tokens = name.split('.')\n",
    "            for token in tokens:\n",
    "                if token.isdigit():\n",
    "                    layer_index = int(token)\n",
    "                    break\n",
    "        \n",
    "        if 'ln' in name or 'norm' in name:\n",
    "            p.requires_grad = not freeze_ln\n",
    "        elif 'embeddings' in name:\n",
    "            p.requires_grad = not freeze_emb\n",
    "        elif 'mlp' in name:\n",
    "            if freeze_ff_layers is None:\n",
    "                # Apply general freeze_ff setting\n",
    "                p.requires_grad = not freeze_ff\n",
    "            else:\n",
    "                # Apply specific layer freeze setting\n",
    "                p.requires_grad = not (freeze_ff and layer_index in freeze_ff_layers)\n",
    "        elif 'attn' in name:\n",
    "            p.requires_grad = not freeze_attn\n",
    "        else:\n",
    "            p.requires_grad = not freeze_other\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba3a21c4-26f5-4ccc-9301-5a015634aabf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vikhr4oDataset(Dataset):\n",
    "    def __init__(self, dataset, tokenizer, quantizer, asr: bool = False):\n",
    "        self.dataset = dataset\n",
    "        self.tokenizer = tokenizer\n",
    "        # if true, sequences of type speech to text\n",
    "        self.asr = asr \n",
    "\n",
    "        self.soa = tokenizer(start_audio_token, return_tensors=\"pt\")[\"input_ids\"][:, -1:].to(device)\n",
    "        self.eoa = tokenizer(end_audio_token, return_tensors=\"pt\")[\"input_ids\"][:, -1:].to(device)\n",
    "        self.eos = tokenizer(end_sequence_token, return_tensors=\"pt\")[\"input_ids\"][:, -1:].to(device)\n",
    "\n",
    "        self.n_original_tokens = len(tokenizer) - 1024\n",
    "        self.quantizer = quantizer \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def quantize(self, example):\n",
    "        audio_data, sample_rate = example[\"audio\"][\"array\"], example[\"audio\"][\"sampling_rate\"]\n",
    "    \n",
    "        # audio -> discrete codes\n",
    "        audio = torch.tensor(audio_data).view(1, 1, len(audio_data)).float()\n",
    "        audio = audio.to(device)\n",
    "        codes = self.quantizer.encode(audio)\n",
    "        codes = codes.squeeze(1)\n",
    "    \n",
    "        # Move tensor back to CPU and delete it to free GPU memory\n",
    "        del audio\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "        # increment tokens' ids \n",
    "        return codes + self.n_original_tokens\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.dataset[idx]\n",
    "\n",
    "        # get text tokens \n",
    "        text = row[\"text\"]\n",
    "        text_tokenized = self.tokenizer(text, return_tensors=\"pt\")\n",
    "        text_input_tokens = text_tokenized[\"input_ids\"].to(device)\n",
    "\n",
    "        # quantize audio \n",
    "        codes = self.quantize(row)\n",
    "\n",
    "        if self.asr:\n",
    "            raw_audio_tokens = codes[:1]\n",
    "            audio_input_tokens = raw_audio_tokens.contiguous().view(1, -1)\n",
    "        else:\n",
    "            raw_audio_tokens = codes[:n_codebooks]\n",
    "            audio_input_tokens = raw_audio_tokens.t().contiguous().view(1, -1)\n",
    "\n",
    "        # determine number of audio tokens given max_seq_length \n",
    "        audio_length = min(max_seq_length - text_input_tokens.shape[-1] - n_special_tokens, audio_input_tokens.shape[-1])\n",
    "\n",
    "        if not self.asr:\n",
    "            audio_length -= audio_length % n_codebooks\n",
    "\n",
    "        padding_size = max_seq_length - text_input_tokens.shape[-1] - audio_length - n_special_tokens\n",
    "        padding = torch.zeros((1, padding_size), dtype=torch.int64, device=device)\n",
    "\n",
    "        if self.asr:\n",
    "            tokens = torch.cat([padding, self.soa, audio_input_tokens[:, :audio_length], self.eoa, text_input_tokens, self.eos], dim=1).squeeze(0)\n",
    "        else:\n",
    "            tokens = torch.cat([padding, text_input_tokens, self.soa, audio_input_tokens[:, :audio_length], self.eoa, self.eos], dim=1).squeeze(0)\n",
    "            \n",
    "        attention_mask = torch.cat([padding, torch.ones((1, max_seq_length - padding_size), device=device)], dim=1).squeeze(0)\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": tokens, \n",
    "            \"attention_mask\": attention_mask, \n",
    "            \"labels\": tokens.clone(),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5cfc5adb-7ee0-4288-b804-39d1c778a552",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alexw/myenv/lib/python3.12/site-packages/torch/nn/utils/weight_norm.py:134: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n",
      "/home/alexw/myenv/lib/python3.12/site-packages/speechtokenizer/model.py:75: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  params = torch.load(ckpt_path, map_location='cpu')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Embedding(33026, 2048)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(base_model, cache_dir=\".\")\n",
    "model = AutoModelForCausalLM.from_pretrained(base_model,\n",
    "                                             attn_implementation=\"sdpa\",\n",
    "                                             cache_dir=\".\")\n",
    "\n",
    "tokenizer.add_special_tokens({'additional_special_tokens': [start_audio_token, end_audio_token]})\n",
    "n_tokens = len(tokenizer)\n",
    "\n",
    "start_audio_token_id = tokenizer(start_audio_token)[\"input_ids\"][-1]\n",
    "end_audio_token_id = tokenizer(end_audio_token)[\"input_ids\"][-1]\n",
    "\n",
    "config_path = \"./audiotokenizer/speechtokenizer_hubert_avg_config.json\"\n",
    "ckpt_path = \"./audiotokenizer/SpeechTokenizer.pt\"\n",
    "quantizer = SpeechTokenizer.load_from_checkpoint(config_path, ckpt_path)\n",
    "quantizer.eval()\n",
    "\n",
    "for n, child in quantizer.named_children():\n",
    "    child.to(device)\n",
    "    child = freeze_entire_model(child)\n",
    "\n",
    "codebook_size = quantizer.quantizer.bins\n",
    "\n",
    "tokenizer.add_tokens([f\"<audio_token_{i}>\" for i in range(codebook_size)])\n",
    "\n",
    "assert len(tokenizer) == n_tokens + codebook_size\n",
    "\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "940d501f-537e-4fe9-ab24-f72e93a435ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_audio_padding_tokens(quantizer):\n",
    "    # create audio without any sounds \n",
    "    # seems to work better than radom padding if \n",
    "    # length of generated audio is not devisible by n_codebooks\n",
    "    audio = torch.zeros((1, 1, 1))\n",
    "    audio = audio.to(device)\n",
    "    \n",
    "    codes = quantizer.encode(audio)\n",
    "\n",
    "    # Move tensor back to CPU and delete it to free GPU memory\n",
    "    del audio\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    return {\"audio_tokens\": codes.squeeze(1)}\n",
    "    \n",
    "\n",
    "\n",
    "def decode_audio(tokens, quantizer, pad_tokens, n_original_tokens, n_codebooks):\n",
    "    # find start and end indices of audio tokens \n",
    "    start = torch.nonzero(tokens == start_audio_token_id)\n",
    "    end = torch.nonzero(tokens == end_audio_token_id)\n",
    "    \n",
    "    start = start[0, -1] + 1 if len(start) else 0\n",
    "    end = end[0, -1] if len(end) else tokens.shape[-1]\n",
    "    \n",
    "    # substract length of original vocabulary -> tokens in range [0, 1024)\n",
    "    audio_tokens = tokens[start:end] % n_original_tokens\n",
    "    reminder = audio_tokens.shape[-1] % n_codebooks\n",
    "    \n",
    "    if reminder:\n",
    "        # pad if last frame is incomplete \n",
    "        audio_tokens = torch.cat([audio_tokens, pad_tokens[reminder:]], dim=0)\n",
    "\n",
    "    if n_codebooks > 1:\n",
    "        transposed = audio_tokens.view(-1, n_codebooks).t()\n",
    "    else:\n",
    "        transposed = audio_tokens\n",
    "    codes = transposed.view(n_codebooks, 1, -1).to(device)\n",
    "\n",
    "    audio = quantizer.decode(codes).squeeze(0)\n",
    "\n",
    "    del tokens \n",
    "    del audio_tokens \n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    return AudioSignal(audio.detach().cpu().numpy(), quantizer.sample_rate)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7583c44e-bb4f-4e06-b8d3-865324a788e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loadiing data\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be643618566f49e0a8bf8738f78d6be4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/45 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def prepare_librispeech():\n",
    "    raw = load_dataset(\"openslr/librispeech_asr\", \"clean\", cache_dir=\".\")\n",
    "    processed = raw.remove_columns([\"chapter_id\"])\n",
    "    processed = processed.cast_column('speaker_id', Value('string'))\n",
    "    return processed \n",
    "\n",
    "\n",
    "def prepare_tedlium():\n",
    "    raw = load_dataset(\"LIUM/tedlium\", \"release1\", cache_dir=\".\")\n",
    "    processed = raw.remove_columns([\"gender\"])\n",
    "    return processed\n",
    "\n",
    "\n",
    "def prepare_parler_tts():    \n",
    "    raw_mls = load_dataset(\"parler-tts/mls_eng\", cache_dir=\"/mnt/storage\")\n",
    "    processed_mls = raw_mls.remove_columns([\"begin_time\", \"end_time\", \"speaker_id\", \"book_id\", \"audio_duration\"])\n",
    "    processed_mls = processed_mls.rename_column('transcript', 'text')\n",
    "\n",
    "    return processed_mls\n",
    "\n",
    "\n",
    "def prepare_synthetic():\n",
    "    raw = load_dataset(\"homebrewltd/instruction-speech-encodec-v1\", cache_dir=\".\")\n",
    "    processed = raw.remove_columns([\"answer\", \"length\"])\n",
    "    processed = processed.rename_column('prompt', 'text')\n",
    "\n",
    "    return processed\n",
    "    \n",
    "\n",
    "load_processed = False\n",
    "\n",
    "if not load_processed:\n",
    "    print(\"Loadiing data\")\n",
    "    if data == \"tedlium\":\n",
    "        dataset = prepare_tedlium()\n",
    "\n",
    "        train_data = dataset[\"train\"]\n",
    "        val_data = dataset[\"validation\"]\n",
    "        \n",
    "    elif data == \"parler-tts\":\n",
    "        dataset = prepare_parler_tts()\n",
    "\n",
    "        train_data = dataset[\"train\"]\n",
    "        val_data = dataset[\"dev\"]\n",
    "        \n",
    "    elif data == \"librispeech\":\n",
    "        dataset = prepare_librispeech()\n",
    "\n",
    "        train_data = dataset[\"train.100\"]\n",
    "        val_data = dataset[\"validation\"]\n",
    "\n",
    "    elif data == \"synthetic\":\n",
    "        dataset = prepare_synthetic()[\"train\"]\n",
    "\n",
    "        splits = dataset.train_test_split(test_size=0.1)\n",
    "        train_data = splits[\"train\"]\n",
    "        val_data = splits[\"test\"]\n",
    "else:\n",
    "    train_data = load_from_disk(os.path.join(path_to_processed, \"train\"))\n",
    "    val_data = load_from_disk(os.path.join(path_to_processed, \"val\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4ee98ebf-aea4-4c32-a6a8-078d25c40cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_tts = Vikhr4oDataset(train_data, tokenizer, quantizer)\n",
    "train_dataset_asr = Vikhr4oDataset(train_data, tokenizer, quantizer, asr=True)\n",
    "\n",
    "val_dataset_tts = Vikhr4oDataset(val_data, tokenizer, quantizer)\n",
    "val_dataset_asr = Vikhr4oDataset(val_data, tokenizer, quantizer, asr=True)\n",
    "\n",
    "train_dataset = ConcatDataset([train_dataset_tts, train_dataset_asr])\n",
    "val_dataset = ConcatDataset([val_dataset_tts, val_dataset_asr])\n",
    "\n",
    "padding_tokens = get_audio_padding_tokens(quantizer)[\"audio_tokens\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4506716e-2768-45f9-936a-5b0f566ff6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(val_dataset) == len(val_dataset_asr) + len(val_dataset_tts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c8e3c254-eda0-4d69-9907-731a9f7c02c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([    0,     0,     0,  ..., 16999,  1525, 29958], device='cuda:0')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_dataset[-1]['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2c7590f2-395c-4d8b-b23a-ef34bc3b6843",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<audiotools.core.audio_signal.AudioSignal at 0x75fb3090cbf0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test audio decoding\n",
    "\n",
    "input_ids_test = val_dataset[0][\"input_ids\"]\n",
    "decoded = decode_audio(input_ids_test, quantizer, padding_tokens, n_tokens, n_codebooks)\n",
    "\n",
    "input_ids_test2 = val_dataset[-1][\"input_ids\"]\n",
    "decoded2 = decode_audio(input_ids_test2, quantizer, padding_tokens, n_tokens, 1)\n",
    "\n",
    "decoded.write(\"tests/test.wav\")\n",
    "decoded2.write(\"tests/test2.wav\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d79cc31-3430-44c3-9727-4a446ea586b0",
   "metadata": {},
   "source": [
    "## No SFT Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54458393-87dd-4f08-bc42-62074c65d35d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mksycheva\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29bdeb68d4b944a19d41fee7b2410165",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011112527088779542, max=1.0â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.8 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/alexw/InternShips/4o/Vikhr4o/wandb/run-20240829_133500-cwahoqo0</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ksycheva/vikhr4o-llama-tiny/runs/cwahoqo0' target=\"_blank\">copper-serenity-8</a></strong> to <a href='https://wandb.ai/ksycheva/vikhr4o-llama-tiny' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ksycheva/vikhr4o-llama-tiny' target=\"_blank\">https://wandb.ai/ksycheva/vikhr4o-llama-tiny</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ksycheva/vikhr4o-llama-tiny/runs/cwahoqo0' target=\"_blank\">https://wandb.ai/ksycheva/vikhr4o-llama-tiny/runs/cwahoqo0</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 57078\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 8\n",
      "  Total optimization steps = 35675\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6880279edb9c4af282500c8af94c6d0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/35675 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
      "/home/alexw/myenv/lib/python3.12/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
     ]
    }
   ],
   "source": [
    "train_batch_size = 1\n",
    "eval_batch_size = 2\n",
    "learning_rate = 5e-4\n",
    "gradient_accumulation_steps = 8\n",
    "lr_scheduler_type = \"cosine\"\n",
    "num_train_epochs = 5\n",
    "num_warmup_steps = 10\n",
    "checkpointing_steps = 1000\n",
    "logging_steps = 20\n",
    "weight_decay = 0.1\n",
    "max_grad_norm = 0.25\n",
    "\n",
    "\n",
    "\n",
    "def test_audio_generation(model, batch, n, quantizer, pad_tokens, n_original_tokens):\n",
    "    inds = random.choices(range(len(batch)), k=n)\n",
    "    audios = []\n",
    "    \n",
    "    for input_ids, attn in batch[\"input_ids\"], batch[\"attention_mask\"]:\n",
    "        with torch.no_grad():\n",
    "            ind = torch.nonzero(input_ids == start_audio_token_id)[0, -1]\n",
    "            input_ids = input_ids[:ind+1].unsqueeze(0)\n",
    "            attn = attn[:ind+1].unsqueeze(0).to(torch.float16)\n",
    "            output = model.generate(input_ids=input_ids, attention_mask=attn ,max_length=max_seq_length)\n",
    "\n",
    "        try:\n",
    "            audio = decode_audio(output, quantizer, pad_tokens, n_original_tokens)\n",
    "            audio_file = os.path.join(save_dir, \"audio\")\n",
    "            os.makedirs(audio_file, exists_ok=True)\n",
    "            audio_file = os.path.join(audio_file, f\"audio_{ind + 1}.wav\")\n",
    "            aduio.write(audio_file)\n",
    "            audios.append(audio_file)\n",
    "        except:\n",
    "            print(\"No audio generated.\")\n",
    "            pass\n",
    "\n",
    "    return audios\n",
    "\n",
    "\n",
    "def get_last_checkpoint():\n",
    "    n_checkpoints = len(list(filter(lambda x: x.startswith(\"checkpoint\"), os.listdir(save_dir))))\n",
    "    return n_checkpoints + 1\n",
    "\n",
    "\n",
    "def save_checkpoint(model, accelerator, tokenizer, optimizer, scheduler):\n",
    "    accelerator.wait_for_everyone()\n",
    "    state = model.state_dict()\n",
    "\n",
    "    path = os.path.join(save_dir, f\"checkpoint-{get_last_checkpoint() * checkpointing_steps}\")\n",
    "    \n",
    "    unwrapped_model = accelerator.unwrap_model(model)\n",
    "    unwrapped_model.save_pretrained(\n",
    "        path, \n",
    "        state_dict=state, \n",
    "        is_main_process=accelerator.is_main_process, \n",
    "        save_function=accelerator.save, \n",
    "        save_embedding_layers=True\n",
    "    )\n",
    "    if accelerator.is_main_process:\n",
    "        tokenizer.save_pretrained(path)\n",
    "        torch.save(optimizer.state_dict(), os.path.join(path, \"optimizer.pt\"))\n",
    "        torch.save(scheduler.state_dict(), os.path.join(path, \"scheduler.pt\"))\n",
    "\n",
    "\n",
    "def train(model, dataloader, accelerator, optimizer, lr_scheduler, completed_steps, progress_bar, max_train_steps):\n",
    "    model.gradient_checkpointing_enable()\n",
    "    model.train()\n",
    "    # model = freeze(model, freeze_ff_layers=None)\n",
    "    total_loss = 0\n",
    "    acc_loss = 0\n",
    "    \n",
    "    for step, batch in enumerate(dataloader): \n",
    "        with accelerator.accumulate(model):\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "    \n",
    "            last_loss = loss.detach().float()\n",
    "            total_loss += last_loss\n",
    "            acc_loss += last_loss \n",
    "            \n",
    "            accelerator.backward(loss)\n",
    "    \n",
    "        if accelerator.sync_gradients:\n",
    "            accelerator.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "            \n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "    \n",
    "            progress_bar.update(1)\n",
    "            completed_steps += 1\n",
    "\n",
    "            acc_loss = acc_loss / gradient_accumulation_steps\n",
    "\n",
    "            accelerator.log({\"loss\": acc_loss.item()})\n",
    "            acc_loss = 0\n",
    "    \n",
    "            if completed_steps % checkpointing_steps == 0:\n",
    "                save_checkpoint(model, accelerator, tokenizer, optimizer, lr_scheduler)\n",
    "\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        if completed_steps >= max_train_steps:\n",
    "                break\n",
    "\n",
    "    return total_loss / len(dataloader), completed_steps\n",
    "\n",
    "\n",
    "def eval(model, dataloader, accelerator, epoch, completed_steps, train_loss, quantizer, pad_tokens, n_original_tokens):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "\n",
    "    eval_progress_bar = tqdm(dataloader, desc=f\"Evaluating Epoch {epoch}\", leave=False)\n",
    "    \n",
    "    for batch in eval_progress_bar:\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "\n",
    "        loss = outputs.loss\n",
    "        losses.append(accelerator.gather_for_metrics(loss.repeat(eval_batch_size)))\n",
    "\n",
    "    losses = torch.cat(losses)\n",
    "    try:\n",
    "        eval_loss = torch.mean(losses)\n",
    "        perplexity = math.exp(eval_loss)\n",
    "    except OverflowError:\n",
    "        perplexity = float(\"inf\")\n",
    "\n",
    "    print(f\"epoch {epoch}: perplexity: {perplexity} eval_loss: {eval_loss}\")\n",
    "    # audios = test_audio_generation(model, batch, 2, quantizer, pad_tokens, n_original_tokens)\n",
    "\n",
    "    base_log = {\n",
    "        \"perplexity\": perplexity,\n",
    "        \"eval_loss\": eval_loss,\n",
    "        \"train_loss\": train_loss.item() / len(train_dataloader),\n",
    "        \"epoch\": epoch,\n",
    "        \"step\": completed_steps,\n",
    "    }\n",
    "    # base_log.update({f\"audio_{i+1}\": audios[i] for i in range(len(audios))})\n",
    "\n",
    "    accelerator.log(base_log, step=completed_steps)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    accelerator = Accelerator(gradient_accumulation_steps=gradient_accumulation_steps, \n",
    "                              mixed_precision='no', \n",
    "                              log_with=\"wandb\")\n",
    "\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset, shuffle=True, collate_fn=default_data_collator, batch_size=train_batch_size\n",
    "    )\n",
    "    eval_dataloader = DataLoader(\n",
    "        val_dataset, collate_fn=default_data_collator, batch_size=eval_batch_size\n",
    "    )\n",
    "\n",
    "    no_decay = [\"bias\", \"layer_norm.weight\"]\n",
    "    optimizer_grouped_parameters = [\n",
    "        {\n",
    "            \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay) and p.requires_grad],\n",
    "            \"weight_decay\": weight_decay,\n",
    "        },\n",
    "        {\n",
    "            \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay) and p.requires_grad],\n",
    "            \"weight_decay\": 0.0,\n",
    "        },\n",
    "    ]\n",
    "    optimizer = torch.optim.AdamW(optimizer_grouped_parameters, lr=learning_rate, fused=True)\n",
    "    # optimizer = bnb.optim.Adam8bit(optimizer_grouped_parameters, min_8bit_size=16384)\n",
    "\n",
    "    num_update_steps_per_epoch = math.ceil(len(train_dataloader) / gradient_accumulation_steps)\n",
    "    max_train_steps = num_train_epochs * num_update_steps_per_epoch\n",
    "\n",
    "    lr_scheduler = get_scheduler(\n",
    "        name=lr_scheduler_type,\n",
    "        optimizer=optimizer,\n",
    "        num_warmup_steps=num_warmup_steps * accelerator.num_processes,\n",
    "        num_training_steps=max_train_steps * accelerator.num_processes,\n",
    "    )\n",
    "\n",
    "    model, optimizer, train_dataloader, eval_dataloader, lr_scheduler = accelerator.prepare(\n",
    "        model, optimizer, train_dataloader, eval_dataloader, lr_scheduler\n",
    "    )\n",
    "\n",
    "    num_update_steps_per_epoch = math.ceil(len(train_dataloader) / gradient_accumulation_steps)\n",
    "    max_train_steps = num_train_epochs * num_update_steps_per_epoch\n",
    "    \n",
    "    num_train_epochs = math.ceil(max_train_steps / num_update_steps_per_epoch)\n",
    "\n",
    "    accelerator.init_trackers(\"vikhr4o-llama-tiny\", {\"lr_scheduler_type\": lr_scheduler_type})\n",
    "\n",
    "    total_batch_size = train_batch_size * accelerator.num_processes * gradient_accumulation_steps\n",
    "\n",
    "    print(\"***** Running training *****\")\n",
    "    print(f\"  Num examples = {len(train_dataset)}\")\n",
    "    print(f\"  Num Epochs = {num_train_epochs}\")\n",
    "    print(f\"  Instantaneous batch size per device = {train_batch_size}\")\n",
    "    print(f\"  Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}\")\n",
    "    print(f\"  Gradient Accumulation steps = {gradient_accumulation_steps}\")\n",
    "    print(f\"  Total optimization steps = {max_train_steps}\")\n",
    "    \n",
    "    progress_bar = tqdm(range(max_train_steps), disable=not accelerator.is_local_main_process)\n",
    "    completed_steps = 0\n",
    "    starting_epoch = 0\n",
    "\n",
    "    for epoch in range(starting_epoch, num_train_epochs):\n",
    "        train_loss, completed_steps = train(model, train_dataloader, accelerator, optimizer, lr_scheduler, completed_steps, progress_bar, max_train_steps)\n",
    "        print(f\"EPOCH {epoch + 1} train loss:\", train_loss)\n",
    "        eval(model, eval_dataloader, accelerator, epoch, completed_steps, train_loss, quantizer, padding_tokens, n_tokens + 1)\n",
    "\n",
    "    save_checkpoint(model, accelerator, tokenizer, optimizer, lr_scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3574c9c1-adc1-4f47-b26a-1a0ed71af7d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
